
   <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
     <channel>
       <title>scikit-learn on Rafael La Buonora</title>
       <link>/tags/scikit-learn/</link>
       <description>Recent content in scikit-learn on Rafael La Buonora</description>
       <generator>Hugo -- gohugo.io</generator>
       <copyright>Copyright &amp;copy; 2019 - Rafael La Buonora</copyright>
       <lastBuildDate>Sun, 16 Jun 2019 00:00:00 +0000</lastBuildDate>
       
           <atom:link href="/tags/scikit-learn/index.xml" rel="self" type="application/rss+xml" />
       
       
       <item>
         <title>Prediciendo Precios de Propiedades</title>
         <link>/posts/predicting-house-prices/</link>
         <pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate>
         
         <guid>/posts/predicting-house-prices/</guid>
         <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#datos-faltantes&#34;&gt;Datos faltantes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#analisis-exploratorio&#34;&gt;Análisis Exploratorio&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#variable-objetivo-saleprice&#34;&gt;Variable objetivo: SalePrice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#outliers&#34;&gt;Outliers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variables-numericas-discontinuas&#34;&gt;Variables numéricas discontinuas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variables-categoricas&#34;&gt;Variables categóricas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variables-categoricas-ordinales&#34;&gt;Variables Categóricas Ordinales&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modelos&#34;&gt;Modelos&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#regresion-sin-regularizacion&#34;&gt;Regresión sin Regularización&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lasso&#34;&gt;Lasso&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ridge&#34;&gt;Ridge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#analisis-de-los-errores&#34;&gt;Análisis de los Errores&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusiones&#34;&gt;Conclusiones&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;En este post estimo tres modelos de regresión para predecir el precio de venta de una propiedad inmobiliaria usando &lt;code&gt;scikit-learn&lt;/code&gt;. Las columnas disponibles incluyen 80 columnas con información sobre la localización de las propiedades, su estructura y estado de conservación, su localización y la fecha y condiciones de la venta.&lt;/p&gt;
&lt;p&gt;Dado que la variable objetivo es continua, uso 3 modelos de regresión: regresión lineal sin regularización, regresión Ridge y regresión Lasso.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Importar módulos
import pandas as pd
import numpy as np
# Leer los datos 
casas = pd.read_csv(&amp;quot;../../public/data/precios_casas/train.csv&amp;quot;)

# Primeras filas
print(casas.head())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Id  MSSubClass MSZoning  ...  SaleType  SaleCondition SalePrice
## 0   1          60       RL  ...        WD         Normal    208500
## 1   2          20       RL  ...        WD         Normal    181500
## 2   3          60       RL  ...        WD         Normal    223500
## 3   4          70       RL  ...        WD        Abnorml    140000
## 4   5          60       RL  ...        WD         Normal    250000
## 
## [5 rows x 81 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cantidad de filas y columnas:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(casas.shape)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (1460, 81)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;datos-faltantes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Datos faltantes&lt;/h2&gt;
&lt;p&gt;Los datos faltantes son un problema habitual en este tipo de tarea. El primer paso para analizarlo es cuantificar cuantos casos tienen datos faltantes en cada columna:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(casas.isnull().any().sum())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 19&lt;/code&gt;&lt;/pre&gt;
&lt;!-- Columnas numéricas con datos faltantes: --&gt;
&lt;!-- ```{python} --&gt;
&lt;!-- # Seleccionar numéricas --&gt;
&lt;!-- numericas = casas.select_dtypes(include = [np.number]) --&gt;
&lt;!-- # Columnas numéricas con faltantes --&gt;
&lt;!-- print(casas[numericas.columns[numericas.isnull().any()]].isnull().sum().sort_values(ascending = False)) --&gt;
&lt;!-- ``` --&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;continuas = [&amp;quot;LotArea&amp;quot;,      &amp;quot;GrLivArea&amp;quot;,   &amp;quot;GarageArea&amp;quot;,
             &amp;quot;WoodDeckSF&amp;quot;,   &amp;quot;OpenPorchSF&amp;quot;, &amp;quot;EnclosedPorch&amp;quot;,
             &amp;quot;3SsnPorch&amp;quot;,    &amp;quot;ScreenPorch&amp;quot;,
             &amp;quot;LowQualFinSF&amp;quot;, &amp;quot;LotFrontage&amp;quot;,
             &amp;quot;1stFlrSF&amp;quot;,     &amp;quot;2ndFlrSF&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;!-- ```{python} --&gt;
&lt;!-- # Columnas categóricas --&gt;
&lt;!-- categoricas = casas.select_dtypes(include=[object]) --&gt;
&lt;!-- # Columnas categóricas con faltantes --&gt;
&lt;!-- print(casas[categoricas.columns[categoricas.isnull().any()]].isnull().sum().sort_values(ascending = False)) --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
&lt;div id=&#34;analisis-exploratorio&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Análisis Exploratorio&lt;/h1&gt;
&lt;div id=&#34;variable-objetivo-saleprice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variable objetivo: SalePrice&lt;/h2&gt;
&lt;p&gt;La variable a predecir es el precio de venta de la propiedad. Es continua y no tiene missings. La propiedad promedio se vendió en $ 180.920, la más barata salió $ 34.900 y la más cara $755.000.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;casas[[&amp;quot;SalePrice&amp;quot;]].describe()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            SalePrice
## count    1460.000000
## mean   180921.195890
## std     79442.502883
## min     34900.000000
## 25%    129975.000000
## 50%    163000.000000
## 75%    214000.000000
## max    755000.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tiene distribución asimétrica, con la cola larga a la derecha (es más probable que una casa sea más cara que más barata que el promedio). Esta asimetría hace que sea conveniente normalizarla para modelarla.&lt;/p&gt;
&lt;p&gt;Defino una función &lt;code&gt;helper_curtosis&lt;/code&gt; para anotar los gráficos de asimetría:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def helper_curtosis(x):
  # Texto sobre la curtosis de x
  # para agregar al gráfico
  sk = skew(x)
  sk_pval = skewtest(x)[0]
  return f&amp;#39;Curtosis:\n{sk:.2f} ({sk_pval:.3f})&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from scipy.stats import skew, skewtest

fig, ax = plt.subplots(1, 2, figsize=(12,4))

# SalePrice sin transformar
x = casas[&amp;quot;SalePrice&amp;quot;]
sns.distplot(x, kde=False, fit=stats.norm, ax=ax[0])
ax[0].text(400000, 0.000005, helper_curtosis(x), fontsize = 14)
#ax[0].set_title(&amp;quot;SalePrice&amp;quot;)

# SalePrice transformada
log1_x = np.log1p(casas[&amp;quot;SalePrice&amp;quot;])
sns.distplot(log1_x, kde=False, fit=stats.norm, ax=ax[1])
ax[1].text(12.6, 0.6, helper_curtosis(log1_x), fontsize = 14)
ax[1].set_xlabel(&amp;quot;log(1+SalePrice)&amp;quot;)

_ = fig.suptitle(&amp;quot;La transformación x = log(1+x) corrige la asimetría en SalePrice&amp;quot;, fontsize=16)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-06-16-prediciendo-precios-propiedades_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;El &lt;a href=&#34;https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.skewtest.html#scipy.stats.skewtest&#34;&gt;test de curtosis&lt;/a&gt; es una prueba estadística para determinar si la distibución es simétrica. En la variable original rechazamos que la distribución sea simétrica y en la transformada con &lt;code&gt;log(1+x)&lt;/code&gt; no rechazamos la hipótesis nula de que la distribución es simétrica.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Correlaciones con SalePrice
corr_sales_price = casas.corr()[&amp;quot;SalePrice&amp;quot;]

# Solo las mayores que 0.63 ordenadas descendiendo
corrs_altas = corr_sales_price[corr_sales_price &amp;gt; 0.4].sort_values(ascending = False)
print(corrs_altas)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## SalePrice       1.000000
## OverallQual     0.790982
## GrLivArea       0.708624
## GarageCars      0.640409
## GarageArea      0.623431
## TotalBsmtSF     0.613581
## 1stFlrSF        0.605852
## FullBath        0.560664
## TotRmsAbvGrd    0.533723
## YearBuilt       0.522897
## YearRemodAdd    0.507101
## GarageYrBlt     0.486362
## MasVnrArea      0.477493
## Fireplaces      0.466929
## Name: SalePrice, dtype: float64&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Calculamos las correlaciones de la variables objetivo con el resto de las variables para determinar en qué variables enfocarnos:&lt;/p&gt;
&lt;p&gt;La calidad de la casa (&lt;code&gt;OverallQual&lt;/code&gt;), el metraje (&lt;code&gt;GrLivArea&lt;/code&gt;), la cantidad de autos que entran en el garage (&lt;code&gt;GarageCars&lt;/code&gt;) y el metraje del garage (&lt;code&gt;GarageArea&lt;/code&gt;) son las variables continuas más importantes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outliers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outliers&lt;/h2&gt;
&lt;p&gt;Hay dos propiedades que tienen valores atípicos.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Marcar outlier
casas[&amp;quot;outlier&amp;quot;] = np.logical_and(casas[&amp;quot;GrLivArea&amp;quot;] &amp;gt; 4000, 
                                  casas[&amp;quot;SalePrice&amp;quot;] &amp;lt; 300000)

# Scatterplot

fig, ax = plt.subplots(1, 2, figsize=(12,4))
_ = sns.scatterplot(x=&amp;quot;GrLivArea&amp;quot;, y=&amp;quot;SalePrice&amp;quot;, hue=&amp;quot;outlier&amp;quot;, data=casas, ax = ax[0])
_ = plt.title(&amp;quot;Dos propiedades baratas para su tamaño&amp;quot;, fontsize = 16)
_ = ax[1].set(xscale=&amp;quot;log&amp;quot;, yscale=&amp;quot;log&amp;quot;)
_ = ax[1].set_xlabel(&amp;quot;log&amp;quot;) #, ylabel = &amp;quot;log&amp;quot;)
_ = sns.scatterplot(x=&amp;quot;GrLivArea&amp;quot;, y=&amp;quot;SalePrice&amp;quot;, hue=&amp;quot;outlier&amp;quot;, data=casas, ax = ax[1])
_ = plt.title(&amp;quot;Transformación logarítmica&amp;quot;, fontsize = 16)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-06-16-prediciendo-precios-propiedades_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variables-numericas-discontinuas&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variables numéricas discontinuas&lt;/h2&gt;
&lt;p&gt;Estas variables son numéricas pero representan cuentas (cantidad de baños, cantidad de cuartos, etc.).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Cantidad de baños, cantidad de garages
cuentas =   [&amp;quot;KitchenAbvGr&amp;quot;, &amp;quot;BedroomAbvGr&amp;quot;, 
             &amp;quot;Fireplaces&amp;quot;,   &amp;quot;BsmtFullBath&amp;quot;,
             &amp;quot;TotRmsAbvGrd&amp;quot;, &amp;quot;FullBath&amp;quot;,
             &amp;quot;HalfBath&amp;quot;,     &amp;quot;YearBuilt&amp;quot;]   

# Panel de (2, 3)
dim_panel = (2, 4)
fig, ax = plt.subplots(dim_panel[0], dim_panel[1], figsize=(12,8))
ax[1,3].tick_params(labelbottom = False)
fig.subplots_adjust(hspace=0.4, wspace=0.4)

# Ordenar los nombres de las columnas (2, 3) para iterar fácil
cols = np.reshape(np.array(cuentas)[:8], dim_panel) # tomo los primeros 6 para que entre en el panel
            
_ = [sns.countplot(x=cols[i, j], data=casas, ax = ax[i, j]) for i in range(dim_panel[0]) 
                                                              for j in range(dim_panel[1])]
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-06-16-prediciendo-precios-propiedades_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variables-categoricas&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variables categóricas&lt;/h2&gt;
&lt;p&gt;Estas variables son categorías y las modelamos como &lt;em&gt;dummies&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;categoricas = [&amp;quot;LotShape&amp;quot;,     &amp;quot;LandContour&amp;quot;,   &amp;quot;BldgType&amp;quot;,   &amp;quot;Foundation&amp;quot;,
               &amp;quot;Neighborhood&amp;quot;, &amp;quot;Exterior1st&amp;quot;,   &amp;quot;LandSlope&amp;quot;,  &amp;quot;HouseStyle&amp;quot;,  
               &amp;quot;PavedDrive&amp;quot;,   &amp;quot;SaleCondition&amp;quot;, &amp;quot;RoofStyle&amp;quot;,  &amp;quot;CentralAir&amp;quot;,     
               &amp;quot;LotShape&amp;quot;,     &amp;quot;LandContour&amp;quot;,   &amp;quot;MSZoning&amp;quot;,   &amp;quot;SaleType&amp;quot;,
               &amp;quot;Street&amp;quot;,       &amp;quot;Utilities&amp;quot;,     &amp;quot;Heating&amp;quot;,    &amp;quot;RoofMatl&amp;quot;,
               &amp;quot;Exterior2nd&amp;quot;,  &amp;quot;LotConfig&amp;quot;,                                     
               &amp;quot;Alley&amp;quot;,        &amp;quot;Electrical&amp;quot;,   &amp;quot;BsmtFinType1&amp;quot;, &amp;quot;BsmtFinType2&amp;quot;,  # Tienen Nan
               &amp;quot;GarageType&amp;quot;,   &amp;quot;MiscFeature&amp;quot;,   &amp;quot;MasVnrType&amp;quot;,  &amp;quot;Fence&amp;quot;          # Tienen Nan
              ]

dim_panel = (5, 5)
fig, ax = plt.subplots(dim_panel[0], dim_panel[1], figsize=(12,16))

# Apagar los tick labels cuando son demasiados
apagar_ejes = [
    (0, 2), (0, 3), (0, 4), 
    (1, 0), (1, 2), (1, 4), 
    (2, 0), (2, 2), (2, 3),
    (3, 0), (3, 3), (3, 4),
    (4, 0), (4, 1), (4, 3), (4, 4)
]

_ = [ax[plt].tick_params(labelbottom = False) for plt in apagar_ejes]

# Fijar los espacios entre los subplots
fig.subplots_adjust(hspace=0.4, wspace=0.6)

cols = np.reshape(np.array(categoricas)[:25], dim_panel)
_ = [sns.countplot(x=cols[i, j], data=casas, ax = ax[i, j]) for i in range(dim_panel[0]) 
                                                              for j in range(dim_panel[1])]
                                                              
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-06-16-prediciendo-precios-propiedades_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Street&lt;/code&gt;, &lt;code&gt;Utilities&lt;/code&gt;, &lt;code&gt;Heating&lt;/code&gt; y &lt;code&gt;RoofMatl&lt;/code&gt; tienen muy poca variación y pueden generar problemas en el modelo.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Sacar las problematicas
cat_problemas = [&amp;quot;Street&amp;quot;, &amp;quot;Utilities&amp;quot;, &amp;quot;Heating&amp;quot;, &amp;quot;RoofMatl&amp;quot;] 

#_ = [ categoricas.remove(col) for col in cat_problemas  ]


cat_missing = [&amp;quot;MasVnrType&amp;quot;,   &amp;quot;GarageType&amp;quot;,  
               #&amp;quot;Electrical&amp;quot;,   &amp;quot;MiscFeature&amp;quot;,
               &amp;quot;BsmtFinType1&amp;quot;, &amp;quot;BsmtFinType2&amp;quot;,
               &amp;quot;LotConfig&amp;quot;,    &amp;quot;Exterior2nd&amp;quot;,  &amp;quot;LandSlope&amp;quot;, 
               &amp;quot;Alley&amp;quot;,        &amp;quot;Fence&amp;quot; ]

_ = [ categoricas.remove(col) for col in cat_missing  ]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;variables-categoricas-ordinales&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variables Categóricas Ordinales&lt;/h2&gt;
&lt;p&gt;El último tipo de columna son las variables categóricas ordinales (ej: calidad de la piscina). En estos casos, la variable aparece como texto, pero en realidad queremos modelarla como numérica, porque Calidad = 2 es más que Calidad = 1. Para eso tenemos que recodificar las columnas.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ordinales = [&amp;quot;OverallQual&amp;quot;,  &amp;quot;OverallCond&amp;quot;, &amp;quot;ExterQual&amp;quot;,
             &amp;quot;ExterCond&amp;quot;,    &amp;quot;BsmtQual&amp;quot;,    &amp;quot;BsmtCond&amp;quot;,
             &amp;quot;BsmtExposure&amp;quot;, &amp;quot;HeatingQC&amp;quot;,   &amp;quot;KitchenQual&amp;quot;,
             &amp;quot;Functional&amp;quot;,   &amp;quot;FireplaceQu&amp;quot;, &amp;quot;PoolQC&amp;quot;,
             &amp;quot;GarageFinish&amp;quot;, &amp;quot;GarageCond&amp;quot;,  &amp;quot;GarageQual&amp;quot;,
             &amp;quot;Fence&amp;quot;] &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Llevamos las que tienen escalas de calidad (Ex, Gd, Fa, Po) a una escala común. Por ejemplo,cuando es &lt;em&gt;missing&lt;/em&gt; la dejamos en 0. Si la calidad de la piscina es missing, es porque la propiedad no tiene piscina y está bien que quede en 0.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def cambiar_escala(df, col, escala):
  # reemplazar missings
  casas[col].fillna(&amp;quot;MISSING&amp;quot;, inplace = True)
  # aplicar escala_ordinal
  casas[col].replace(escala, inplace = True)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Po &amp;lt; Fa &amp;lt; TA &amp;lt; Gd &amp;lt; Ex
ESCALA_ORDINAL = { &amp;quot;MISSING&amp;quot;: 0, &amp;quot;Po&amp;quot; : 1, &amp;quot;Fa&amp;quot; : 2, 
                   &amp;quot;TA&amp;quot; : 3,     &amp;quot;Gd&amp;quot; : 4, &amp;quot;Ex&amp;quot; : 5}

ordinales_escala_comun = [ &amp;quot;ExterQual&amp;quot;,   &amp;quot;ExterCond&amp;quot;,   
                           &amp;quot;BsmtQual&amp;quot;,    &amp;quot;BsmtCond&amp;quot;,     
                           &amp;quot;HeatingQC&amp;quot;,   &amp;quot;KitchenQual&amp;quot;,  
                           &amp;quot;FireplaceQu&amp;quot;, &amp;quot;PoolQC&amp;quot;,       
                           &amp;quot;GarageCond&amp;quot;  ]

# cambiar casas inplace
_ = [ cambiar_escala(casas, col, ESCALA_ORDINAL) for col in ordinales_escala_comun]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ESCALA_FUNCTIONAL = { &amp;quot;Sal&amp;quot; : 0, &amp;quot;Sev&amp;quot; : 1,
                      &amp;quot;Maj2&amp;quot;: 2, &amp;quot;Maj1&amp;quot;: 3,
                      &amp;quot;Mod&amp;quot; : 4, &amp;quot;Min2&amp;quot;: 5,
                      &amp;quot;Min1&amp;quot;: 6, &amp;quot;Typ&amp;quot;: 7
                    }

cambiar_escala(casas, &amp;quot;Functional&amp;quot;, escala = ESCALA_FUNCTIONAL)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Panel con las variables ordinales
# TODO refactorear a una funcion con las de las cuentas
dim_panel = (4, 4)
fig, ax = plt.subplots(dim_panel[0], dim_panel[1], figsize=(16,18))


# Fijar los espacios entre los subplots
fig.subplots_adjust(hspace=0.3, wspace=0.6)

cols = np.reshape(np.array(ordinales), dim_panel)
_ = [sns.countplot(x=cols[i, j], data=casas, ax = ax[i, j]) for i in range(dim_panel[0]) 
                                                              for j in range(dim_panel[1])]

plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-06-16-prediciendo-precios-propiedades_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1536&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Posibles problemas con &lt;code&gt;PoolQC&lt;/code&gt; y &lt;code&gt;Functional&lt;/code&gt;. &lt;code&gt;BsmtCond&lt;/code&gt; y &lt;code&gt;GarageCond&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Missing en &lt;code&gt;PoolQC&lt;/code&gt; (1453) , &lt;code&gt;FireplaceQu&lt;/code&gt; (690) , &lt;code&gt;GarageCond&lt;/code&gt; (81) , &lt;code&gt;GarageQual&lt;/code&gt; (81), &lt;code&gt;GarageFinish&lt;/code&gt; (81), &lt;code&gt;GarageType&lt;/code&gt;(81), &lt;code&gt;BsmtExposure&lt;/code&gt; (38) , &lt;code&gt;BsmtFinType1&lt;/code&gt; (37), &lt;code&gt;BsmtCond&lt;/code&gt; (37), &lt;code&gt;BsmtQual&lt;/code&gt; (37).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ordinales_missing = [&amp;quot;PoolQC&amp;quot;,       &amp;quot;FireplaceQu&amp;quot;, &amp;quot;GarageCond&amp;quot;,
                     &amp;quot;GarageQual&amp;quot;,    
                     &amp;quot;BsmtCond&amp;quot;,
                     &amp;quot;BsmtQual&amp;quot;]     
ordinales_problemas = []   # Problematicas
ordinales_escala    = [&amp;quot;Fence&amp;quot;, &amp;quot;BsmtExposure&amp;quot;, &amp;quot;GarageFinish&amp;quot;, &amp;quot;ExterCond&amp;quot;]   # Estos los saco pq no arme la escala

_ = [ ordinales.remove(col) for col in ordinales_escala + ordinales_missing]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Para evitar problemas de multicolinealidad nos quedamos sólo con &lt;code&gt;TotalBsmtSF&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Extraer X e y como arrays
y = casas[&amp;quot;SalePrice&amp;quot;]
X = casas[ordinales + categoricas + continuas + cuentas ]
columnas_con_missing = X.columns[X.isnull().any()]

print(f&amp;#39;Columnas con missings para imputar: {columnas_con_missing}&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Columnas con missings para imputar: Index([&amp;#39;Electrical&amp;#39;, &amp;#39;MiscFeature&amp;#39;, &amp;#39;LotFrontage&amp;#39;], dtype=&amp;#39;object&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV
from sklearn.compose import TransformedTargetRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import make_scorer, mean_absolute_error
from sklearn.model_selection import cross_val_score

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, RobustScaler, FunctionTransformer, PolynomialFeatures

transform_categoricas = Pipeline(steps=[
    # Imputar faltantes con el valor mas comun y despues hacer dummies
    (&amp;#39;imp&amp;#39;, SimpleImputer(strategy=&amp;quot;most_frequent&amp;quot;)),
    (&amp;#39;onehot&amp;#39;, OneHotEncoder(handle_unknown=&amp;quot;ignore&amp;quot;))                                                 
])

transform_continuas = Pipeline(steps = [
    (&amp;#39;imp_cont&amp;#39;, SimpleImputer(strategy=&amp;#39;mean&amp;#39;)),
    (&amp;#39;trans&amp;#39;, FunctionTransformer(np.log1p, validate=True)),
    (&amp;#39;scale&amp;#39;, StandardScaler())
])

transform_ordinales = Pipeline(steps = [
   (&amp;#39;ord&amp;#39;, OrdinalEncoder())
])


transform_polys = Pipeline(steps = [
   (&amp;#39;polys&amp;#39;, PolynomialFeatures(2))
])


# ColumnTransformer aplica las pipelines en paralelo y concatena las columnas resultantes
preprocesador = ColumnTransformer(
  transformers=[
#     (&amp;#39;polys&amp;#39;, transform_polys, [&amp;quot;GrLivArea&amp;quot;]),
      (&amp;#39;cat&amp;#39;, transform_categoricas, categoricas),
      (&amp;#39;num&amp;#39;, transform_continuas, continuas + cuentas),
      (&amp;#39;ord&amp;#39;,   transform_ordinales, ordinales)
#     (&amp;#39;cnts&amp;#39;, &amp;#39;passthrough&amp;#39;, cuentas ) 
  ]
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;modelos&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modelos&lt;/h1&gt;
&lt;p&gt;Estimamos tres modelos: regresión lineal estándar, regresión Ridge y regresión Lasso. La diferencia entre los tres modelos es la forma de regularizar los coeficientes. En el caso de la regresión estándar, se minimiza la suma de cuadrados de los residuos: &lt;span class=&#34;math inline&#34;&gt;\(\sum (y - \hat{y})^2\)&lt;/span&gt;. Ridge y Lasso penalizan la norma del vector de coeficientes &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Esto evita el &lt;em&gt;ovefitting&lt;/em&gt; a los datos que usamos para entrenar el modelo.&lt;/p&gt;
&lt;p&gt;En el caso de la regresión Ridge, se agrega un término a la función de costos un término que incluye la norma &lt;span class=&#34;math inline&#34;&gt;\(L_2\)&lt;/span&gt; del vector de coeficientes estimados. La regresión Lasso penaliza la norma &lt;span class=&#34;math inline&#34;&gt;\(L_1\)&lt;/span&gt;. Ambos métodos incluyen un parámetro &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; que pondera el costo de la norma del vector de coeficientes en la función de costos del modelo. Para determinar el valor de este parámetro, uso Cross-Validation con 5 folds.&lt;/p&gt;
&lt;p&gt;Hay dos métricas posibles para evaluar los modelos de regresión: &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_absolute_error&#34;&gt;Mean Absolute&lt;/a&gt; y &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_squared_error&#34;&gt;Mean Squared Error&lt;/a&gt;. El MSE penaliza más los outliers, pero no está medido en la misma unidad que la variable objetivo, por lo que vamos a usar el MAE, que si se mide en la unidad de la variable de destino (en este caso son dólares)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;scorer = make_scorer(mean_absolute_error)
X_prep = preprocesador.fit_transform(X) # TODO: este paso debería estar adentro del 
                                        #       pipeline pero no funciona &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;regresion-sin-regularizacion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regresión sin Regularización&lt;/h2&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;reg = TransformedTargetRegressor(regressor=LinearRegression(),
                                        func=np.log1p,
                                        inverse_func=np.expm1)

cv_linreg = cross_val_score(reg, X_prep, y, cv = 5, scoring = scorer)
print(f&amp;#39;MAE Sin Reg: ${cv_linreg.mean():.2f}&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## MAE Sin Reg: $16604.95&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lasso&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lasso&lt;/h2&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
lassoCV = TransformedTargetRegressor(regressor=LassoCV(cv = 5),
                                        func=np.log1p,
                                        inverse_func=np.expm1)

lassoCV_score = cross_val_score(lassoCV, X_prep, y, cv = 5, scoring = scorer)

print(f&amp;#39;MAE Lasso: ${lassoCV_score.mean():.2f}&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## MAE Lasso: $15978.56&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;lassoCV.fit(X_prep, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## TransformedTargetRegressor(check_inverse=True, func=&amp;lt;ufunc &amp;#39;log1p&amp;#39;&amp;gt;,
##               inverse_func=&amp;lt;ufunc &amp;#39;expm1&amp;#39;&amp;gt;,
##               regressor=LassoCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,
##     max_iter=1000, n_alphas=100, n_jobs=None, normalize=False,
##     positive=False, precompute=&amp;#39;auto&amp;#39;, random_state=None,
##     selection=&amp;#39;cyclic&amp;#39;, tol=0.0001, verbose=False),
##               transformer=None)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(f&amp;#39;Mejor Alpha: {lassoCV.regressor_.alpha_:.2f}&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Mejor Alpha: 0.00&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ridge&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ridge&lt;/h2&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
ridgeCV = TransformedTargetRegressor(regressor=RidgeCV(cv = 5),
                                        func=np.log1p,
                                        inverse_func=np.expm1)

ridgeCV_score = cross_val_score(ridgeCV, X_prep, y, cv = 5, scoring = scorer)
print(f&amp;#39;MAE Ridge: ${ridgeCV_score.mean():.2f}&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## MAE Ridge: $16463.24&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ridgeCV.fit(X_prep, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## TransformedTargetRegressor(check_inverse=True, func=&amp;lt;ufunc &amp;#39;log1p&amp;#39;&amp;gt;,
##               inverse_func=&amp;lt;ufunc &amp;#39;expm1&amp;#39;&amp;gt;,
##               regressor=RidgeCV(alphas=array([ 0.1,  1. , 10. ]), cv=5, fit_intercept=True,
##     gcv_mode=None, normalize=False, scoring=None, store_cv_values=False),
##               transformer=None)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(f&amp;#39;Mejor Alpha: {ridgeCV.regressor_.alpha_}&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Mejor Alpha: 10.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;analisis-de-los-errores&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Análisis de los Errores&lt;/h2&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fig, ax = plt.subplots(1, 3, figsize=(16, 4))
reg.fit(X_prep, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## TransformedTargetRegressor(check_inverse=True, func=&amp;lt;ufunc &amp;#39;log1p&amp;#39;&amp;gt;,
##               inverse_func=&amp;lt;ufunc &amp;#39;expm1&amp;#39;&amp;gt;,
##               regressor=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,
##          normalize=False),
##               transformer=None)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;preds = reg.predict(X_prep)
preds = pd.DataFrame({&amp;quot;preds&amp;quot;:preds, &amp;quot;true&amp;quot;:y})
preds[&amp;quot;residuals&amp;quot;] = preds[&amp;quot;true&amp;quot;] - preds[&amp;quot;preds&amp;quot;]
_ = preds.plot(x = &amp;quot;preds&amp;quot;, y = &amp;quot;residuals&amp;quot;, kind = &amp;quot;scatter&amp;quot;, ax = ax[0])
_ = ax[0].set_title(&amp;quot;Sin Regularización&amp;quot;)

preds = lassoCV.predict(X_prep)
preds = pd.DataFrame({&amp;quot;preds&amp;quot;:preds, &amp;quot;true&amp;quot;:y})
preds[&amp;quot;residuals&amp;quot;] = preds[&amp;quot;true&amp;quot;] - preds[&amp;quot;preds&amp;quot;]
_ = preds.plot(x = &amp;quot;preds&amp;quot;, y = &amp;quot;residuals&amp;quot;, kind = &amp;quot;scatter&amp;quot;, ax = ax[1])
_ = ax[1].set_title(&amp;quot;Lasso&amp;quot;)


preds = ridgeCV.predict(X_prep)
preds = pd.DataFrame({&amp;quot;preds&amp;quot;:preds, &amp;quot;true&amp;quot;:y})
preds[&amp;quot;residuals&amp;quot;] = preds[&amp;quot;true&amp;quot;] - preds[&amp;quot;preds&amp;quot;]
_ = preds.plot(x = &amp;quot;preds&amp;quot;, y = &amp;quot;residuals&amp;quot;, kind = &amp;quot;scatter&amp;quot;, ax = ax[2])
_ = ax[2].set_title(&amp;quot;Ridge&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-06-16-prediciendo-precios-propiedades_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;1536&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Los errores de los tres modelos muestran que hay margen para mejora porque los errores de predicción son sistemáticamente altos para valores alto de la predicción. Agregar cuadrados de los features podría mejorar los resultados.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusiones&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusiones&lt;/h1&gt;
&lt;p&gt;Entrenamos 3 modelos y elegimos los hiperparámetros con un esquema de Cross-Validation con 5 &lt;em&gt;folds&lt;/em&gt;. El mejor modelo es la regresión Lasso con &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.00045\)&lt;/span&gt;, que permite predecir el precio de una vivienda con un error promedio de aproximadamente $ 15.100, menos del 10% de la media de la variable objetivo ($181.000). Dada la alta cantidad de columnas colineales en los regresores, era de esperar que Lasso funcionara mejor, porque tiende a hace que muchos de los coeficientes &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; estimados sean 0.&lt;/p&gt;
&lt;/div&gt;
</description>
       </item>
       
     </channel>
   </rss>
